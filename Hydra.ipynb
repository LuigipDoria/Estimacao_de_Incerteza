{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# LIBRARYs\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import wandb\n",
    "\n",
    "from Utils.treino import *\n",
    "from Utils.init_redes import *\n",
    "from Utils.redes import *\n",
    "from Utils.data_set import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'  \n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "data_set = \"Cifar100\"\n",
    "\n",
    "if data_set == \"Cifar10\":\n",
    "    transform_train, transform_test, batch_size, trainset, trainloader, testset, testloader, classes = cifar10()\n",
    "elif data_set == \"Cifar100\":\n",
    "    transform_train, transform_test, batch_size, trainset, trainloader, testset, testloader= cifar100()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_epoch = 120\n",
    "\n",
    "\n",
    "num_cabeca = 15\n",
    "net, loss_criterion, optimizer_net, scheduler_net = init_rede_all(\"Resnet100_hydra\", num_cabeca)\n",
    "\n",
    "name = \"Hydra_{}_cabecas\".format(num_cabeca)\n",
    "\n",
    "config = dict (\n",
    "  architecture = \"Resnet18 Hydra\",\n",
    "  N_cabeças = num_cabeca,\n",
    "  Data_set = data_set,\n",
    "  epocas = num_epoch,\n",
    ")\n",
    "\n",
    "wandb.init(project=\"Hydra\", config=config, name=\"{} Cabeças\".format(num_cabeca), group=data_set)\n",
    "\n",
    "best_acc = 0\n",
    "best_loss = 100\n",
    "for epoch in range(start_epoch, num_epoch):\n",
    "    print(epoch, end=\" \")\n",
    "    train_loss, train_acc = train(epoch=epoch, model=net, loss_criterion=loss_criterion, optimizer=optimizer_net, scheduler=scheduler_net,trainloader=trainloader)\n",
    "    test_loss, test_acc, best_acc, best_loss = test(epoch=epoch, model=net, loss_criterion=loss_criterion, optimizer=optimizer_net, scheduler=scheduler_net, best_acc=best_acc, best_loss=best_loss,name=name,testloader=testloader)\n",
    "    print(\"Train acc: {} | Train Loss: {} || Teste acc: {} | Teste Loss: {}\".format(train_acc, train_loss, test_acc, test_loss))\n",
    "    wandb.log({\"epoch\": epoch, \"Loss Treino\": train_loss, \"acuracia treino\": train_acc, \"acuracia teste\": test_acc, \"Loss Teste\": test_loss})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "hydra, _,__,___  = init_rede_all(\"Resnet100_hydra\", num_cabeca)\n",
    "hydra.load_state_dict(torch.load(\"./Resultado_parciais/{}_acc.pth\".format(name)))\n",
    "\n",
    "acc_teste = testa_acuracia(hydra, testloader)\n",
    "torch.save(hydra.state_dict(), ('./Data_sets/Hydra {}/Hydra_{}_cabecas_acc_{}.pth'.format(data_set,num_cabeca,acc_teste)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluigidoria\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/luigi-doria/IC/wandb/run-20230306_104837-car0oy1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/luigidoria/Hydra/runs/car0oy1y\" target=\"_blank\">2 Cabeças</a></strong> to <a href=\"https://wandb.ai/luigidoria/Hydra\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/luigidoria/Hydra\" target=\"_blank\">https://wandb.ai/luigidoria/Hydra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/luigidoria/Hydra/runs/car0oy1y\" target=\"_blank\">https://wandb.ai/luigidoria/Hydra/runs/car0oy1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train acc: 8.316 | Train Loss: 1556.262865304947 || Teste acc: 12.51 | Teste Loss: 369.599609375\n",
      "1 "
     ]
    }
   ],
   "source": [
    "num_epoch = 120\n",
    "\n",
    "data_sets = [\"Cifar100\", \"Cifar10\"]\n",
    "for data_set in data_sets:\n",
    "    if data_set == \"Cifar10\":\n",
    "        transform_train, transform_test, batch_size, trainset, trainloader, testset, testloader, classes = cifar10()\n",
    "        rede = \"Resnet_hydra\"\n",
    "        max_ = 16\n",
    "    elif data_set == \"Cifar100\":\n",
    "        transform_train, transform_test, batch_size, trainset, trainloader, testset, testloader= cifar100()\n",
    "        rede = \"Resnet100_hydra\"\n",
    "        max_ = 15\n",
    " \n",
    "    for i in range(2,max_):\n",
    "        num_cabeca = i\n",
    "        net, loss_criterion, optimizer_net, scheduler_net = init_rede_all(rede, num_cabeca)\n",
    "\n",
    "        name = \"Hydra_{}_cabecas\".format(num_cabeca)\n",
    "\n",
    "        config = dict (\n",
    "          architecture = \"Resnet18 Hydra\",\n",
    "          N_cabeças = num_cabeca,\n",
    "          Data_set = data_set,\n",
    "          epocas = num_epoch,\n",
    "        )\n",
    "\n",
    "        wandb.init(project=\"Hydra\", config=config, name=\"{} Cabeças\".format(num_cabeca), group=data_set)\n",
    "\n",
    "        best_acc = 0\n",
    "        best_loss = 100\n",
    "        for epoch in range(start_epoch, num_epoch):\n",
    "            print(epoch, end=\" \")\n",
    "            train_loss, train_acc = train(epoch=epoch, model=net, loss_criterion=loss_criterion, optimizer=optimizer_net, scheduler=scheduler_net,trainloader=trainloader)\n",
    "            test_loss, test_acc, best_acc, best_loss = test(epoch=epoch, model=net, loss_criterion=loss_criterion, optimizer=optimizer_net, scheduler=scheduler_net, best_acc=best_acc, best_loss=best_loss,name=name,testloader=testloader)\n",
    "            print(\"Train acc: {} | Train Loss: {} || Teste acc: {} | Teste Loss: {}\".format(train_acc, train_loss, test_acc, test_loss))\n",
    "            wandb.log({\"epoch\": epoch, \"Loss Treino\": train_loss, \"acuracia treino\": train_acc, \"acuracia teste\": test_acc, \"Loss Teste\": test_loss})\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        hydra, _,__,___  = init_rede_all(rede, num_cabeca)\n",
    "        hydra.load_state_dict(torch.load(\"./Resultado_parciais/{}_acc.pth\".format(name)))\n",
    "\n",
    "        acc_teste = testa_acuracia(hydra, testloader)\n",
    "        torch.save(hydra.state_dict(), ('./Data_sets/Hydra {}/Hydra_{}_cabecas_acc_{}.pth'.format(data_set,num_cabeca,acc_teste)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
